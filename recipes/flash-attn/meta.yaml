{% set name = "flash-attn" %}
{% set version = "2.5.8" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/flash_attn-{{ version }}.tar.gz
  sha256: 2e5b2bcff6d5cff40d494af91ecd1eb3c5b4520a6ce7a0a8b1f9c1ed129fb402

build:
  number: 0
  script: {{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation
  script_env:
    - FLASH_ATTENTION_FORCE_BUILD=TRUE
    - FLASH_ATTENTION_SKIP_CUDA_BUILD=FALSE
    - FLASH_ATTENTION_FORCE_CXX11_ABI=FALSE
    - MAX_JOBS=$CPU_COUNT
  skip: true  # [cuda_compiler_version in (undefined, "None")]
  skip: true  # [not linux]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    - {{ stdlib('c') }}
    - ninja
  host:
    - cuda-version {{ cuda_compiler_version }}
    - libcublas-dev    # [(cuda_compiler_version or "").startswith("12")]
    - libcusolver-dev  # [(cuda_compiler_version or "").startswith("12")]
    - libcusparse-dev  # [(cuda_compiler_version or "").startswith("12")]
    - packaging
    - pip
    - psutil
    - python
    - pytorch
    - pytorch =*=cuda*
    - libtorch  # required until pytorch run_exports libtorch
    - setuptools
    - wheel
  run:
    - pytorch =*=cuda*
    - python
    - einops

test:
  imports:
    - flash_attn
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/Dao-AILab/flash-attention
  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'
  license: BSD-3-Clause
  license_file: LICENSE

extra:
  recipe-maintainers:
    - weiji14
